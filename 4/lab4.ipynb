{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 4: topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной лабораторной работе мы попытаемся обучить LDA-модель topic-моделингу на двух принципиально различных корпусах. \n",
    "\n",
    "В первой части вы познакомитесь с новыми возможностями библиотеки gensim, а также с возможностями парсинга в языке Python. Во второй части вам предстоит самостоятельно обучить LDA-модель и оценить качество её работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1: topic modeling уровня /b/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Краеугольным камнем в машинном обучений в целом, и в NLP в частности, является выбор датасетов. Доселе мы использовали только стандартные, многократно обкатанные датасеты, но сегодня попробуем собрать свой. Практика работы с сырыми, необработанными данными весьма полезно! Заодно изучим возможности парсеров в Питоне.\n",
    "\n",
    "Давайте напишем парсер, собирающий информацию о сообщения с русскомязычного анононимного форума (имиджборды) \"Двач\" (\"Сосач\", \"Хиккач\", если вам угодно). Двач, как и всякая имиджборда разделён на разделы (доски, борды), посвященные различным тематикам -- аниме, видеоигры, литература, религия... Каждая доска состоит из тем (тредов, топиков), которые создаются анонимными (при их желании) пользователями. Каждый тред посвящен обсуждению какого-то конкретного вопроса.\n",
    "\n",
    "У некоторых разделов есть раздел архив, располагается он по адресу https://2ch.hk/(название раздела)/arch/, например для раздела музыка -- https://2ch.hk/mu/arch/. Если у вас есть минимальные навыки в языке html, а также если вы изучили документацию встроенного класса HTMLParser, то вам будет несложно написать два парсера.\n",
    "\n",
    "Первый парсер (ArchiveParser) парсит HTML-страницу архива доски, вытягивает из неё ссылки на заархивированные треды, и скармливает их второму парсеру.\n",
    "\n",
    "Второй парсер (ThreadParser) парсит HTML-страницу заархивированного треда, вытягивает из неё сообщения, складывает их вместе и собирает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "from html.parser import HTMLParser\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def get_value_by_key(attrs, key):\n",
    "    for (k, v) in attrs:\n",
    "        if(k == key):\n",
    "            return v;\n",
    "    return None\n",
    "\n",
    "class ArchiveParser(HTMLParser):\n",
    "    flag = False\n",
    "    threads = []\n",
    "    limit = 200\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if(self.limit > 0):\n",
    "            if(tag == 'div'):\n",
    "                cl = get_value_by_key(attrs, 'class')\n",
    "                if (cl == 'box-data'):\n",
    "                    self.flag = True;\n",
    "            if(self.flag == True and tag == 'a'):\n",
    "                href = get_value_by_key(attrs, 'href')\n",
    "                if(len(href)>20):\n",
    "                    print(href)\n",
    "                    print(self.limit)\n",
    "                    thread = parse_thread('https://2ch.hk' + href)\n",
    "                    if(len(thread) > 10):\n",
    "                        self.threads.append(thread)\n",
    "                        self.limit = self.limit - 1\n",
    "                    thread = []\n",
    "        \n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if(tag == 'div'):\n",
    "            self.flag = False;\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        1+1\n",
    "        \n",
    "    def get_threads(self):\n",
    "        return self.threads\n",
    "    \n",
    "    def clean(self):\n",
    "        self.threads = []\n",
    "        \n",
    "parser = ArchiveParser()\n",
    "\n",
    "def parse_archive(board = '/b/', page_number = 0):\n",
    "    lines = []\n",
    "    link = 'https://2ch.hk' + board + 'arch/' + str(page_number) +'.html'\n",
    "    print(link)\n",
    "    parser.limit = 100\n",
    "    url = urllib.request.urlopen(link)\n",
    "    for line in url.readlines():\n",
    "        lines.append(line.decode('utf-8'))\n",
    "    for line in lines:\n",
    "        parser.feed(line)\n",
    "    res = parser.get_threads()\n",
    "    parser.clean()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreadParser(HTMLParser):\n",
    "    flag = False\n",
    "    message = []\n",
    "    messages = []\n",
    "            \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if(tag == 'blockquote'):\n",
    "            self.flag = True;\n",
    "            self.message = []\n",
    "        \n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if(tag == 'blockquote'):\n",
    "            self.flag = False\n",
    "            if(self.message != []):\n",
    "                self.messages.append(self.message)\n",
    "            self.message = []\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if(self.flag):\n",
    "            self.message.extend(simple_preprocess(data))\n",
    "            \n",
    "    def get_messages(self):\n",
    "        return self.messages\n",
    "    \n",
    "    def clear_messages(self):\n",
    "        flag = False\n",
    "        self.message = []\n",
    "        self.messages = []\n",
    "\n",
    "t_parser = ThreadParser()\n",
    "\n",
    "def parse_thread (link):\n",
    "    url = urllib.request.urlopen(link)\n",
    "    lines = []\n",
    "    for line in url.readlines():\n",
    "        lines.append(line.decode('utf-8', errors='ignore'))\n",
    "    for line in lines:\n",
    "        t_parser.feed(line)\n",
    "    res = t_parser.get_messages()\n",
    "    t_parser.clear_messages()\n",
    "    #print(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весьма много кода, верно? Если не потерялись, могли заметить функцию parse_archive, которая парсит страницу архива по доске и номеру страницы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Задание.}$\n",
    "Давайте применим её к каким-нибудь доскам. Выберите две доски двача, имеющие архив и скачайте архивы функцией parse_archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://2ch.hk/pr/arch/0.html\n",
      "/pr/arch/2016-05-04/res/708924.html\n",
      "100\n",
      "/pr/arch/2016-05-03/res/708740.html\n",
      "100\n",
      "/pr/arch/2016-05-11/res/708703.html\n",
      "100\n",
      "/pr/arch/2016-05-03/res/708541.html\n",
      "99\n",
      "/pr/arch/2016-05-02/res/708338.html\n",
      "99\n",
      "/pr/arch/2016-05-02/res/708024.html\n",
      "99\n",
      "/pr/arch/2016-05-02/res/708023.html\n",
      "99\n",
      "/pr/arch/2016-05-02/res/708010.html\n",
      "99\n",
      "/pr/arch/2016-05-09/res/707960.html\n",
      "98\n",
      "/pr/arch/2016-05-12/res/707882.html\n",
      "97\n",
      "/pr/arch/2016-05-03/res/707795.html\n",
      "96\n",
      "/pr/arch/2016-05-04/res/707611.html\n",
      "95\n",
      "/pr/arch/2016-05-11/res/707568.html\n",
      "94\n",
      "/pr/arch/2016-05-02/res/707426.html\n",
      "93\n",
      "/pr/arch/2016-06-08/res/707395.html\n",
      "92\n",
      "/pr/arch/2016-05-01/res/707282.html\n",
      "91\n",
      "/pr/arch/2016-05-11/res/707083.html\n",
      "91\n",
      "/pr/arch/2016-05-02/res/707079.html\n",
      "90\n",
      "/pr/arch/2016-05-01/res/707032.html\n",
      "89\n",
      "/pr/arch/2016-04-30/res/707031.html\n",
      "89\n",
      "/pr/arch/2016-05-01/res/706875.html\n",
      "89\n",
      "/pr/arch/2016-05-07/res/706843.html\n",
      "88\n",
      "/pr/arch/2016-05-02/res/706697.html\n",
      "87\n",
      "/pr/arch/2016-04-29/res/706407.html\n",
      "87\n",
      "/pr/arch/2016-07-26/res/706304.html\n",
      "87\n",
      "/pr/arch/2016-04-29/res/706251.html\n",
      "86\n",
      "/pr/arch/2016-05-05/res/705863.html\n",
      "86\n",
      "/pr/arch/2016-05-07/res/705826.html\n",
      "85\n",
      "/pr/arch/2016-05-04/res/705586.html\n",
      "84\n",
      "/pr/arch/2016-07-24/res/705167.html\n",
      "83\n",
      "/pr/arch/2016-05-01/res/704977.html\n",
      "82\n",
      "/pr/arch/2016-04-29/res/704949.html\n",
      "81\n",
      "/pr/arch/2016-04-30/res/704930.html\n",
      "80\n",
      "/pr/arch/2016-05-04/res/704664.html\n",
      "80\n",
      "/pr/arch/2016-04-30/res/704598.html\n",
      "79\n",
      "/pr/arch/2016-05-20/res/704545.html\n",
      "79\n",
      "/pr/arch/2016-05-04/res/704490.html\n",
      "78\n",
      "/pr/arch/2016-04-28/res/704328.html\n",
      "77\n",
      "/pr/arch/2016-04-27/res/704132.html\n",
      "77\n",
      "/pr/arch/2016-05-27/res/704120.html\n",
      "77\n",
      "/pr/arch/2016-05-10/res/704022.html\n",
      "76\n",
      "/pr/arch/2016-05-03/res/703923.html\n",
      "75\n",
      "/pr/arch/2016-05-17/res/703890.html\n",
      "74\n",
      "/pr/arch/2016-04-27/res/703877.html\n",
      "73\n",
      "/pr/arch/2016-05-12/res/703806.html\n",
      "73\n",
      "/pr/arch/2016-04-28/res/703767.html\n",
      "72\n",
      "/pr/arch/2016-05-04/res/703542.html\n",
      "71\n",
      "/pr/arch/2016-04-29/res/703462.html\n",
      "70\n",
      "/pr/arch/2016-04-26/res/703430.html\n",
      "69\n",
      "/pr/arch/2016-04-26/res/703135.html\n",
      "69\n",
      "/pr/arch/2016-05-04/res/702800.html\n",
      "68\n",
      "/pr/arch/2016-04-25/res/702701.html\n",
      "67\n",
      "/pr/arch/2016-04-30/res/702133.html\n",
      "66\n",
      "/pr/arch/2016-05-27/res/702072.html\n",
      "65\n",
      "/pr/arch/2016-04-25/res/701775.html\n",
      "64\n",
      "/pr/arch/2016-04-28/res/701733.html\n",
      "64\n",
      "/pr/arch/2016-04-25/res/701678.html\n",
      "63\n",
      "/pr/arch/2016-04-29/res/701538.html\n",
      "63\n",
      "/pr/arch/2016-04-25/res/701368.html\n",
      "62\n",
      "/pr/arch/2016-04-25/res/701169.html\n",
      "62\n",
      "/pr/arch/2016-04-25/res/701150.html\n",
      "62\n",
      "/pr/arch/2016-04-25/res/701039.html\n",
      "61\n",
      "/pr/arch/2016-04-25/res/701033.html\n",
      "60\n",
      "/pr/arch/2016-04-29/res/700777.html\n",
      "59\n",
      "/pr/arch/2016-03-27/res/700597.html\n",
      "58\n",
      "/pr/arch/2016-03-27/res/700370.html\n",
      "58\n",
      "/pr/arch/2016-04-26/res/700286.html\n",
      "58\n",
      "/pr/arch/2016-03-27/res/700147.html\n",
      "57\n",
      "/pr/arch/2016-04-25/res/699912.html\n",
      "57\n",
      "/pr/arch/2016-03-27/res/699896.html\n",
      "56\n",
      "/pr/arch/2016-03-27/res/699877.html\n",
      "56\n",
      "/pr/arch/2016-03-27/res/699653.html\n",
      "56\n",
      "/pr/arch/2016-03-27/res/699617.html\n",
      "56\n",
      "/pr/arch/2016-04-25/res/699344.html\n",
      "55\n",
      "/pr/arch/2016-03-26/res/699281.html\n",
      "55\n",
      "/pr/arch/2016-05-20/res/699142.html\n",
      "55\n",
      "/pr/arch/2016-03-26/res/699001.html\n",
      "54\n",
      "/pr/arch/2016-04-25/res/698817.html\n",
      "53\n",
      "/pr/arch/2016-04-26/res/698574.html\n",
      "53\n",
      "/pr/arch/2016-03-26/res/698357.html\n",
      "52\n",
      "/pr/arch/2016-03-25/res/698287.html\n",
      "51\n",
      "/pr/arch/2016-12-07/res/697835.html\n",
      "51\n",
      "/pr/arch/2016-03-25/res/697594.html\n",
      "50\n",
      "/pr/arch/2016-03-25/res/697592.html\n",
      "50\n",
      "/pr/arch/2016-03-25/res/697496.html\n",
      "49\n",
      "/pr/arch/2016-03-24/res/697381.html\n",
      "49\n",
      "/pr/arch/2016-03-24/res/697101.html\n",
      "49\n",
      "/pr/arch/2016-03-24/res/696877.html\n",
      "48\n",
      "/pr/arch/2016-03-24/res/696858.html\n",
      "47\n",
      "/pr/arch/2016-03-23/res/696620.html\n",
      "47\n",
      "/pr/arch/2016-04-24/res/696401.html\n",
      "46\n",
      "/pr/arch/2016-03-23/res/696137.html\n",
      "45\n",
      "/pr/arch/2016-03-23/res/696032.html\n",
      "45\n",
      "/pr/arch/2016-03-22/res/695576.html\n",
      "44\n",
      "/pr/arch/2016-03-22/res/695498.html\n",
      "43\n",
      "/pr/arch/2016-03-22/res/695260.html\n",
      "43\n",
      "/pr/arch/2016-05-01/res/694892.html\n",
      "43\n",
      "/pr/arch/2016-03-21/res/694678.html\n",
      "42\n",
      "/pr/arch/2016-03-21/res/694267.html\n",
      "41\n",
      "/pr/arch/2016-07-25/res/694248.html\n",
      "41\n",
      "/pr/arch/2016-03-21/res/694162.html\n",
      "40\n",
      "/pr/arch/2016-03-21/res/694033.html\n",
      "40\n",
      "/pr/arch/2016-04-27/res/693879.html\n",
      "39\n",
      "/pr/arch/2016-03-20/res/693723.html\n",
      "38\n",
      "/pr/arch/2016-03-20/res/693700.html\n",
      "37\n",
      "/pr/arch/2016-03-20/res/693550.html\n",
      "37\n",
      "/pr/arch/2016-03-20/res/693167.html\n",
      "37\n",
      "/pr/arch/2016-04-27/res/693065.html\n",
      "36\n",
      "/pr/arch/2016-04-25/res/692721.html\n",
      "35\n",
      "/pr/arch/2016-06-11/res/692538.html\n",
      "34\n",
      "/pr/arch/2016-03-19/res/692400.html\n",
      "33\n",
      "/pr/arch/2016-03-19/res/692136.html\n",
      "32\n",
      "/pr/arch/2016-03-18/res/692090.html\n",
      "31\n",
      "/pr/arch/2016-06-09/res/691428.html\n",
      "30\n",
      "/pr/arch/2016-05-04/res/690877.html\n",
      "29\n",
      "/pr/arch/2016-03-16/res/690281.html\n",
      "28\n",
      "/pr/arch/2016-03-16/res/690259.html\n",
      "27\n",
      "/pr/arch/2016-03-16/res/690217.html\n",
      "26\n",
      "/pr/arch/2016-04-27/res/689843.html\n",
      "26\n",
      "/pr/arch/2016-03-16/res/689676.html\n",
      "25\n",
      "/pr/arch/2016-03-16/res/689537.html\n",
      "25\n",
      "/pr/arch/2016-03-16/res/689508.html\n",
      "24\n",
      "/pr/arch/2016-03-15/res/689273.html\n",
      "24\n",
      "/pr/arch/2016-03-15/res/689244.html\n",
      "24\n",
      "/pr/arch/2016-03-15/res/689205.html\n",
      "24\n",
      "/pr/arch/2016-03-15/res/689040.html\n",
      "24\n",
      "/pr/arch/2016-03-15/res/688646.html\n",
      "24\n",
      "/pr/arch/2016-03-15/res/688633.html\n",
      "23\n",
      "/pr/arch/2016-03-15/res/688615.html\n",
      "22\n",
      "/pr/arch/2016-03-15/res/688544.html\n",
      "22\n",
      "/pr/arch/2016-03-15/res/688521.html\n",
      "22\n",
      "/pr/arch/2016-03-14/res/688279.html\n",
      "21\n",
      "/pr/arch/2016-03-14/res/688223.html\n",
      "21\n",
      "/pr/arch/2016-03-14/res/688036.html\n",
      "21\n",
      "/pr/arch/2016-03-14/res/688009.html\n",
      "21\n",
      "/pr/arch/2016-03-14/res/687853.html\n",
      "21\n",
      "/pr/arch/2016-03-14/res/687803.html\n",
      "20\n",
      "/pr/arch/2016-03-14/res/687571.html\n",
      "19\n",
      "/pr/arch/2016-03-14/res/687417.html\n",
      "18\n",
      "/pr/arch/2016-03-14/res/687393.html\n",
      "17\n",
      "/pr/arch/2016-06-14/res/687373.html\n",
      "16\n",
      "/pr/arch/2016-03-14/res/687347.html\n",
      "15\n",
      "/pr/arch/2016-03-14/res/687286.html\n",
      "15\n",
      "/pr/arch/2016-03-14/res/687254.html\n",
      "15\n",
      "/pr/arch/2016-03-14/res/687176.html\n",
      "14\n",
      "/pr/arch/2016-03-13/res/686206.html\n",
      "14\n",
      "/pr/arch/2016-05-11/res/685932.html\n",
      "14\n",
      "/pr/arch/2016-03-12/res/685658.html\n",
      "13\n",
      "/pr/arch/2016-03-12/res/685522.html\n",
      "12\n",
      "/pr/arch/2016-05-17/res/685455.html\n",
      "12\n",
      "/pr/arch/2016-03-12/res/685377.html\n",
      "11\n",
      "/pr/arch/2016-03-12/res/685345.html\n",
      "11\n",
      "/pr/arch/2016-03-12/res/685131.html\n",
      "10\n",
      "/pr/arch/2016-03-12/res/684951.html\n",
      "10\n",
      "/pr/arch/2016-03-12/res/684627.html\n",
      "9\n",
      "/pr/arch/2016-03-12/res/684577.html\n",
      "9\n",
      "/pr/arch/2016-03-11/res/684460.html\n",
      "9\n",
      "/pr/arch/2016-04-29/res/684115.html\n",
      "8\n",
      "/pr/arch/2016-03-11/res/684056.html\n",
      "7\n",
      "/pr/arch/2016-03-11/res/684010.html\n",
      "6\n",
      "/pr/arch/2016-03-11/res/683899.html\n",
      "5\n",
      "/pr/arch/2016-03-11/res/683894.html\n",
      "4\n",
      "/pr/arch/2016-03-11/res/683861.html\n",
      "4\n",
      "/pr/arch/2016-03-11/res/683589.html\n",
      "4\n",
      "/pr/arch/2016-05-17/res/683573.html\n",
      "4\n",
      "/pr/arch/2016-03-10/res/683079.html\n",
      "3\n",
      "/pr/arch/2016-03-10/res/683012.html\n",
      "3\n",
      "/pr/arch/2016-03-10/res/682755.html\n",
      "3\n",
      "/pr/arch/2016-03-10/res/682716.html\n",
      "3\n",
      "/pr/arch/2016-03-10/res/682578.html\n",
      "3\n",
      "/pr/arch/2016-03-10/res/682494.html\n",
      "2\n",
      "/pr/arch/2016-03-10/res/682165.html\n",
      "1\n",
      "/pr/arch/2016-03-10/res/682072.html\n",
      "1\n",
      "https://2ch.hk/ftb/arch/0.html\n",
      "/ftb/arch/2016-07-01/res/835602.html\n",
      "100\n",
      "/ftb/arch/2016-07-01/res/835074.html\n",
      "99\n",
      "/ftb/arch/2016-06-30/res/834512.html\n",
      "98\n",
      "/ftb/arch/2016-06-30/res/833911.html\n",
      "97\n",
      "/ftb/arch/2016-06-30/res/833363.html\n",
      "96\n",
      "/ftb/arch/2016-06-30/res/832847.html\n",
      "95\n",
      "/ftb/arch/2016-06-30/res/832327.html\n",
      "94\n",
      "/ftb/arch/2016-06-29/res/832264.html\n",
      "93\n",
      "/ftb/arch/2016-06-29/res/831742.html\n",
      "93\n",
      "/ftb/arch/2016-06-29/res/829819.html\n",
      "92\n",
      "/ftb/arch/2016-06-28/res/829814.html\n",
      "91\n",
      "/ftb/arch/2016-06-28/res/829216.html\n",
      "90\n",
      "/ftb/arch/2016-06-28/res/828546.html\n",
      "89\n",
      "/ftb/arch/2016-06-28/res/827815.html\n",
      "88\n",
      "/ftb/arch/2016-06-27/res/827219.html\n",
      "87\n",
      "/ftb/arch/2016-06-27/res/826627.html\n",
      "86\n",
      "/ftb/arch/2016-06-27/res/826005.html\n",
      "85\n",
      "/ftb/arch/2016-06-27/res/825127.html\n",
      "84\n",
      "/ftb/arch/2016-06-27/res/825084.html\n",
      "83\n",
      "/ftb/arch/2016-06-27/res/824371.html\n",
      "82\n",
      "/ftb/arch/2016-06-27/res/823556.html\n",
      "81\n",
      "/ftb/arch/2016-06-27/res/822997.html\n",
      "80\n",
      "/ftb/arch/2016-06-26/res/822289.html\n",
      "79\n",
      "/ftb/arch/2016-06-27/res/822253.html\n",
      "79\n",
      "/ftb/arch/2016-06-26/res/821683.html\n",
      "78\n",
      "/ftb/arch/2016-06-26/res/820995.html\n",
      "77\n",
      "/ftb/arch/2016-06-26/res/820450.html\n",
      "76\n",
      "/ftb/arch/2016-06-26/res/819725.html\n",
      "75\n",
      "/ftb/arch/2016-11-26/res/817771.html\n",
      "74\n",
      "/ftb/arch/2016-06-25/res/817060.html\n",
      "73\n",
      "/ftb/arch/2016-06-25/res/816498.html\n",
      "72\n",
      "/ftb/arch/2016-06-25/res/816486.html\n",
      "72\n",
      "/ftb/arch/2016-06-25/res/815835.html\n",
      "72\n",
      "/ftb/arch/2016-06-25/res/815776.html\n",
      "72\n",
      "/ftb/arch/2016-07-13/res/815373.html\n",
      "71\n",
      "/ftb/arch/2016-12-07/res/815172.html\n",
      "70\n",
      "/ftb/arch/2016-07-11/res/815002.html\n",
      "69\n",
      "/ftb/arch/2016-08-01/res/814863.html\n",
      "68\n",
      "/ftb/arch/2016-06-25/res/814819.html\n",
      "67\n",
      "/ftb/arch/2016-07-02/res/814558.html\n",
      "66\n",
      "/ftb/arch/2016-08-27/res/814114.html\n",
      "65\n",
      "/ftb/arch/2016-06-23/res/813935.html\n",
      "64\n",
      "/ftb/arch/2016-06-22/res/813596.html\n",
      "64\n",
      "/ftb/arch/2016-06-23/res/813569.html\n",
      "63\n",
      "/ftb/arch/2016-06-21/res/813029.html\n",
      "62\n",
      "/ftb/arch/2016-06-21/res/812485.html\n",
      "61\n",
      "/ftb/arch/2016-08-12/res/811911.html\n",
      "60\n",
      "/ftb/arch/2016-06-24/res/811757.html\n",
      "59\n",
      "/ftb/arch/2016-06-21/res/811587.html\n",
      "58\n",
      "/ftb/arch/2016-06-21/res/810704.html\n",
      "57\n",
      "/ftb/arch/2016-06-21/res/810099.html\n",
      "56\n",
      "/ftb/arch/2016-06-20/res/809969.html\n",
      "55\n",
      "/ftb/arch/2016-06-21/res/809499.html\n",
      "55\n",
      "/ftb/arch/2016-06-21/res/808759.html\n",
      "54\n",
      "/ftb/arch/2016-06-20/res/808193.html\n",
      "53\n",
      "/ftb/arch/2016-06-20/res/807927.html\n",
      "52\n",
      "/ftb/arch/2016-06-20/res/806951.html\n",
      "51\n",
      "/ftb/arch/2016-06-19/res/806301.html\n",
      "50\n",
      "/ftb/arch/2016-06-19/res/805777.html\n",
      "49\n",
      "/ftb/arch/2016-06-19/res/805543.html\n",
      "48\n",
      "/ftb/arch/2016-06-20/res/805103.html\n",
      "47\n",
      "/ftb/arch/2017-12-26/res/804777.html\n",
      "46\n",
      "/ftb/arch/2016-06-18/res/804434.html\n",
      "45\n",
      "/ftb/arch/2016-06-18/res/803670.html\n",
      "44\n",
      "/ftb/arch/2016-06-18/res/802915.html\n",
      "43\n",
      "/ftb/arch/2016-08-24/res/802257.html\n",
      "42\n",
      "/ftb/arch/2016-07-07/res/802154.html\n",
      "41\n",
      "/ftb/arch/2016-06-18/res/802028.html\n",
      "40\n",
      "/ftb/arch/2016-06-17/res/801263.html\n",
      "39\n",
      "/ftb/arch/2016-06-24/res/801184.html\n",
      "38\n",
      "/ftb/arch/2016-06-17/res/800401.html\n",
      "37\n",
      "/ftb/arch/2016-06-17/res/799564.html\n",
      "36\n",
      "/ftb/arch/2016-06-17/res/798958.html\n",
      "35\n",
      "/ftb/arch/2017-12-26/res/798730.html\n",
      "34\n",
      "/ftb/arch/2016-06-17/res/798394.html\n",
      "33\n",
      "/ftb/arch/2016-06-16/res/797842.html\n",
      "32\n",
      "/ftb/arch/2016-06-16/res/797313.html\n",
      "31\n",
      "/ftb/arch/2016-07-03/res/797091.html\n",
      "30\n",
      "/ftb/arch/2016-06-16/res/797021.html\n",
      "30\n",
      "/ftb/arch/2016-06-16/res/796517.html\n",
      "29\n",
      "/ftb/arch/2016-06-22/res/796473.html\n",
      "28\n",
      "/ftb/arch/2016-06-16/res/794824.html\n",
      "27\n",
      "/ftb/arch/2016-06-16/res/794713.html\n",
      "26\n",
      "/ftb/arch/2016-06-16/res/793981.html\n",
      "25\n",
      "/ftb/arch/2016-06-15/res/793201.html\n",
      "24\n",
      "/ftb/arch/2016-06-16/res/792786.html\n",
      "23\n",
      "/ftb/arch/2016-06-26/res/792346.html\n",
      "22\n",
      "/ftb/arch/2016-06-15/res/792187.html\n",
      "21\n",
      "/ftb/arch/2016-06-17/res/791542.html\n",
      "20\n",
      "/ftb/arch/2016-06-15/res/791438.html\n",
      "19\n",
      "/ftb/arch/2016-06-15/res/790568.html\n",
      "18\n",
      "/ftb/arch/2016-07-29/res/789405.html\n",
      "17\n",
      "/ftb/arch/2016-06-15/res/789206.html\n",
      "16\n",
      "/ftb/arch/2016-06-15/res/788292.html\n",
      "15\n",
      "/ftb/arch/2016-06-13/res/787219.html\n",
      "14\n",
      "/ftb/arch/2016-06-15/res/786918.html\n",
      "13\n",
      "/ftb/arch/2016-06-19/res/786907.html\n",
      "12\n",
      "/ftb/arch/2016-06-15/res/786035.html\n",
      "11\n",
      "/ftb/arch/2016-06-13/res/785646.html\n",
      "10\n",
      "/ftb/arch/2016-08-12/res/785344.html\n",
      "9\n",
      "/ftb/arch/2016-06-18/res/785051.html\n",
      "8\n",
      "/ftb/arch/2016-06-12/res/784852.html\n",
      "7\n",
      "/ftb/arch/2016-06-12/res/784300.html\n",
      "6\n",
      "/ftb/arch/2016-06-22/res/784278.html\n",
      "5\n",
      "/ftb/arch/2016-06-15/res/783773.html\n",
      "4\n",
      "/ftb/arch/2016-06-13/res/783745.html\n",
      "3\n",
      "/ftb/arch/2016-06-12/res/782665.html\n",
      "2\n",
      "/ftb/arch/2016-06-12/res/782091.html\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#TODO: напишите название досок в формате /'доска'/, например /mu/ для Музыки\n",
    "boards = ['/pr/','/ftb/'] \n",
    "threads_by_topic = [parse_archive(board=board) for board in boards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши данны на тренировочые и тестовые. Пусть каждый десятый тред попадает в тест-сет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "test = []\n",
    "\n",
    "it = 0\n",
    "for topic in threads_by_topic:\n",
    "    for thread in topic:\n",
    "        full = []\n",
    "        for post in thread:\n",
    "            full.extend(post)\n",
    "        it = it + 1\n",
    "        if(it % 10 == 0):\n",
    "            test.append(full)\n",
    "        else:\n",
    "            data.append(full)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Задание.}$\n",
    "В русском языке есть множество слов (частицы, междометия, всё что вы хотите), которые никак не отображают смысл слов и являются вспомогательными. Чтобы ваша модель работала лучше -- добавьте стоп-слова в список RUSSIAN_STOP_WORDS или в строку st_str. Эти слова отфильтруются из датасета перед тем, как модель начнет обучаться на датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vladiknaska/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "_ = nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "russian_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "\n",
    "RUSSIAN_STOP_WORDS = ['не', 'это', 'что','чем','это','как',\n",
    "                      'https','нет','op','он','же','так','но',\n",
    "                      'да','нет','или','и', 'на', \"то\", \"бы\", \n",
    "                      \"все\", \"ты\", \"если\", \"по\", \"за\", \"там\", \n",
    "                      \"ну\", \"уже\", \"от\", \"есть\",\"был\", \"даже\", \n",
    "                      \"было\", \"www\", \"com\", \"youtube\", \"из\",\n",
    "                      \"будет\", \"mp\", \"они\", \"только\", \"его\", \"она\", \n",
    "                      \"вот\", 'просто', 'watch', 'кто', 'для', \n",
    "                      'когда', 'тут', 'мне', 'где', 'мы', 'какой',\n",
    "                      'может', 'меня', 'до', 'про', 'http', 'раз',\n",
    "                      'почему', 'тебя', 'ещё', 'их', 'сейчас',\n",
    "                      'тоже', 'во', 'чтобы', 'этого','без', 'него',\n",
    "                      'вы','такой', 'можно', 'надо', 'нахуй', 'ли',\n",
    "                      'потом', 'тред', 'больше', 'лучше', 'хуй', \n",
    "                      'сам', 'после', 'со', 'лол', 'быть', 'нужно', \n",
    "                      'этом', 'блять', 'бля', 'того', 'ничего', \n",
    "                      'потому', 'нибудь', 'этот', 'под', 'через', \n",
    "                      'ни', 'себе', 'ему', 'при', 'какие', 'пиздец',\n",
    "                      'теперь', 'хоть', 'говно', 'тогда', 'блядь', \n",
    "                      'кстати', 'че', 'себя', 'конечно', 'типа', \n",
    "                      'много', 'том', 'нихуя', 'куда', 'всегда', 'нас', \n",
    "                      'тот', 'ведь', 'эти', 'них', 'сука', 'пока', \n",
    "                      'более', 'чего', 'html', 'были', 'всех',\n",
    "                      'была', 'например', 'тем', 'ru', 'зачем', \n",
    "                      'либо', 'вроде', 'всего', 'вопрос', 'php', \n",
    "                      'против', 'здесь', 'ее', 'значит', 'совсем', \n",
    "                      'сколько', 'им', 'org', 'именно', 'эту']\n",
    "\n",
    "st_str = ('которых которые твой которой которого сих ком свой твоя этими'\n",
    "          ' слишком нами всему будь саму чаще ваше сами наш затем еще'\n",
    "          ' самих наши ту каждое мочь весь этим наша своих оба который'\n",
    "          ' зато те этих вся ваш такая теми ею которая нередко каждая '\n",
    "          'также чему собой самими нем вами ими откуда такие тому та '\n",
    "          'очень сама нему алло оно этому кому тобой таки твоё каждые '\n",
    "          'твои мой нею самим ваши ваша кем мои однако сразу свое ними '\n",
    "          'всё неё тех хотя всем тобою тебе одной другие этао само эта '\n",
    "          'буду самой моё своей такое всею будут своего кого свои мог '\n",
    "          'нам особенно её самому наше кроме вообще вон мною никто это')\n",
    "\n",
    "RUSSIAN_STOP_WORDS.extend(st_str.split(' '))\n",
    "RUSSIAN_STOP_WORDS.extend(russian_stopwords)\n",
    "\n",
    "data = [list(filter(lambda word: not word in RUSSIAN_STOP_WORDS, piece)) for piece in data]\n",
    "id2word = corpora.Dictionary(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим словарь и на его основе преобразуем слова в их id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим LDA-модель, используя библиотеку gensim. Зададим число тем равно числу скачанных досок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "model = LdaModel(corpus, id2word=id2word, num_topics=len(threads_by_topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь получим топ-10 самых используемых в каждой теме слов.\n",
    "\n",
    "$\\textbf{Задание.}$\n",
    "Оцените насколько хорошо модель разделила темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['код', 'время', 'делать', 'сделать', 'динамо', 'года', 'россии', 'евро', 'матч', 'лет']\n",
      "['лет', 'сделать', 'код', 'россии', 'время', 'делать', 'евро', 'россия', 'сборной', 'работает']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(threads_by_topic)):\n",
    "    print([id2word[id[0]] for id in model.get_topic_terms(topicid = i, topn = 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь прогоним тестовые треды на модели. Тестовый датасет разделен на n равных частей по 20 тредов, i-ая соответствует i-й доске."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.2870672), (1, 0.7129328)]\n"
     ]
    }
   ],
   "source": [
    "other_corpus = [id2word.doc2bow(text) for text in [list(filter(lambda word: not word in RUSSIAN_STOP_WORDS, piece)) for piece in test]]\n",
    "\n",
    "vector = [model[unseen_doc] for unseen_doc in other_corpus]\n",
    "print(vector[0]) #вероятности принадлежности 0-го тестового треда в ту или иную тему"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text #0, topic #1, prob = 0.7129328\n",
      "Text #1, topic #0, prob = 0.7574346\n",
      "Text #2, topic #1, prob = 0.6016699\n",
      "Text #3, topic #1, prob = 0.5844992\n",
      "Text #4, topic #1, prob = 0.62229055\n",
      "Text #5, topic #1, prob = 0.5133282\n",
      "Text #6, topic #0, prob = 0.6036269\n",
      "Text #7, topic #1, prob = 0.88308245\n",
      "Text #8, topic #1, prob = 0.6279996\n",
      "Text #9, topic #1, prob = 0.5479618\n",
      "Text #10, topic #0, prob = 0.5413816\n",
      "Text #11, topic #0, prob = 0.5538754\n",
      "Text #12, topic #0, prob = 0.7733906\n",
      "Text #13, topic #1, prob = 0.59811765\n",
      "Text #14, topic #0, prob = 0.73121154\n",
      "Text #15, topic #0, prob = 0.6736597\n",
      "Text #16, topic #0, prob = 0.69949967\n",
      "Text #17, topic #0, prob = 0.6734555\n",
      "Text #18, topic #0, prob = 0.7103451\n",
      "Text #19, topic #0, prob = 0.7124265\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for res in vector:\n",
    "    max_it = 0\n",
    "    if(len(res) > 0):\n",
    "        for it in range(1, len(res)):\n",
    "            if(res[max_it][1] < res[it][1]):\n",
    "                max_it = it\n",
    "        print(\"Text #\" + str(i) + \", topic #\" + str(max_it) + str(\", prob = \" + str(res[max_it][1])))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Задание.}$\n",
    "\n",
    "Оцените результаты работы модели на тест сете. Если модель разделили данные плохо -- объясните, почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. А теперь нормальный датасет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь давайте воспользуемся более стандартным датасетом библиотеки sklreatn -- 20newsgroups, посвященную статьям на различные темы. Выберем 6 -- Атеизм, яблочное железо, автомобили, хоккей, космос, христианство, ближний восток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'rec.autos',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.mideast']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                      remove=('headers', 'footers', 'quotes'), \n",
    "                                      categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'rec.autos',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.mideast']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Задание}$\n",
    "\n",
    "Найдите библиотечный или опишите свой список ENGSLISH_STOP_WORDS, убирающий не несущие никакого смысла английские слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "ENGLISH_STOP_WORDS = stopwords.words('english') #TODO\n",
    "\n",
    "print('the' in ENGLISH_STOP_WORDS) \n",
    "\n",
    "data = [list(filter(lambda word: not word in ENGLISH_STOP_WORDS, simple_preprocess(piece)))\n",
    "                    for piece in newsgroups_train.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Большое задание 1.}$\n",
    "\n",
    "Для списка data создайте словарь id2word. Получите преобразованный TermDocumentFrequency список corpust и обучите на нем LDA модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['considering', 'adding', 'floptical', 'drive', 'current', 'system', 'would', 'like', 'know', 'floptical', 'drives', 'recommended', 'quality', 'performance', 'preference', 'would', 'floptical', 'drives', 'capable', 'handling', 'floppies', 'handling', 'floppies', 'necessity', 'far', 'know', 'bit', 'iomega', 'floptical', 'infinity', 'floptical', 'drives', 'comments', 'recommendations', 'either', 'floptical', 'drives', 'worth', 'looking', 'purchased', 'mail', 'order', 'places', 'etc', 'thanks', 'advance', 'please', 'send', 'replies', 'directly', 'umsoroko', 'ccu', 'umanitoba', 'ca']\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 4), (12, 1), (13, 1), (14, 1), (15, 2), (16, 6), (17, 2), (18, 1), (19, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 2)]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel, LsiModel\n",
    "from gensim import corpora\n",
    "\n",
    "print(data[0])\n",
    "id2word = corpora.Dictionary(data)\n",
    "# Create Corpus\n",
    "texts = data\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1])\n",
    "\n",
    "model = LdaModel(corpus, id2word=id2word, \n",
    "                 num_topics=len(categories), iterations=400) #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alt.Atheism: \n",
      "['finnish', 'chart', 'abstinence', 'checking', 'laws', 'one', 'reasonable', 'failure', 'however', 'wake', 'catholic', 'representation', 'invaders', 'warrant', 'evils', 'seems', 'bottom', 'would', 'predicted', 'world', 'armies', 'country', 'government', 'religious', 'everywhere']\n",
      "Comp.Sys.Mac.Hardware: \n",
      "['think', 'gt', 'manta', 'rate', 'use', 'sure', 'pretty', 'early', 'sold', 'name', 'kadett', 'would', 'like', 'actual', 'one', 'top', 'sex', 'students', 'even', 'mid', 'list', 'bottom', 'high', 'impressed', 'control']\n",
      "Rec.Autos: \n",
      "['turkey', 'govern', 'volunteers', 'rate', 'kurds', 'failure', 'bristol', 'caucasus', 'armenian', 'ability', 'pot', 'russian', 'arfa', 'top', 'irregular', 'prevention', 'ambassador', 'power', 'safest', 'one', 'london', 'disaster', 'source', 'eastern', 'races']\n",
      "Rec.Sport.Hockey: \n",
      "['ottawa', 'would', 'surprise', 'smileys', 'absolute', 'psuvm', 'hirsch', 'shades', 'rangers', 'team', 'cares', 'american', 'go', 'could', 'one', 'edu', 'space', 'patrick', 'nothing', 'taste', 'eh', 'finnish', 'think', 'lindros', 'hartford']\n",
      "Sci.Space: \n",
      "['people', 'would', 'bible', 'one', 'go', 'think', 'many', 'someone', 'could', 'point', 'religious', 'way', 'christian', 'never', 'look', 'well', 'become', 'said', 'get', 'nothing', 'course', 'jesus', 'reasonable', 'right', 'believe']\n",
      "Soc.Religion.Christian: \n",
      "['tony', 'conform', 'pontiac', 'fingers', 'australia', 'warriors', 'would', 'calculations', 'one', 'sentiments', 'european', 'mediocrity', 'aztecs', 'people', 'documentary', 'think', 'conventional', 'nazi', 'ranting', 'get', 'mid', 'billion', 'traded', 'many', 'could']\n",
      "Talk.Politics.Mideast: \n",
      "['lindros', 'battalions', 'pinnacle', 'team', 'poorly', 'league', 'surprise', 'rangers', 'corey', 'goalie', 'hartford', 'trade', 'shutout', 'mail', 'maybe', 'quebec', 'earn', 'best', 'keith', 'played', 'season', 'hockey', 'nhl', 'leafs', 'edu']\n"
     ]
    }
   ],
   "source": [
    "#Выведем получившийся список тем:\n",
    "for i, cat in enumerate(categories):\n",
    "    print(f'{cat.title()}: ')\n",
    "    print([id2word[idx[0]] for idx in model.get_topic_terms(topicid=i, topn=25)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Большое задание 2.}$\n",
    "\n",
    "В соответствии с тренировочными, обработайте тестовые данные.\n",
    "\n",
    "Напишите функцию, которая с помощью модели возвращает наиболее вероятный id темы. С помощью F-меры оцените правильность работы модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                     remove=('headers', 'footers', 'quotes'), \n",
    "                                     categories = categories)\n",
    "#TODO: YOUD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_corpus(dataset, stopwords):\n",
    "    data = [list(filter(lambda word: not word in stopwords, simple_preprocess(piece))) \\\n",
    "            for piece in dataset.data]\n",
    "    id2word = corpora.Dictionary(data)\n",
    "    corpus = [id2word.doc2bow(text) for text in data]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = create_test_corpus(newsgroups_test, ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text):\n",
    "    vector = model[text]\n",
    "    return max(vector, key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_f_score(per, y_pred, y_true):\n",
    "    d = {0:per[0], 1:per[1], 2:per[2], 3:per[3], \n",
    "         4:per[4], 5:per[5], 6:per[6]}\n",
    "    new_vec = [d.get(n, n) for n in y_pred]\n",
    "    f1 = f1_score(y_true, new_vec, average='weighted')\n",
    "#     acc = accuracy_score(y_true, new_vec)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5040/5040 [00:37<00:00, 133.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "permuts = list(itertools.permutations([0, 1, 2, 3, 4, 5, 6]))\n",
    "y_pred = [predict(model, sample)[0] for sample in test_corpus]\n",
    "y_true = newsgroups_test['target'].tolist()\n",
    "\n",
    "res = {}\n",
    "for per in tqdm(permuts):\n",
    "    res[per] = find_f_score(per, y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1696403211641113"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "res_perm = max(res.items(), key=operator.itemgetter(1))[0]\n",
    "res[res_perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sci.Space: \n",
      "['finnish', 'chart', 'abstinence', 'checking', 'laws', 'one', 'reasonable', 'failure', 'however', 'wake', 'catholic', 'representation', 'invaders', 'warrant', 'evils', 'seems', 'bottom', 'would', 'predicted', 'world', 'armies', 'country', 'government', 'religious', 'everywhere']\n",
      "Rec.Autos: \n",
      "['think', 'gt', 'manta', 'rate', 'use', 'sure', 'pretty', 'early', 'sold', 'name', 'kadett', 'would', 'like', 'actual', 'one', 'top', 'sex', 'students', 'even', 'mid', 'list', 'bottom', 'high', 'impressed', 'control']\n",
      "Talk.Politics.Mideast: \n",
      "['turkey', 'govern', 'volunteers', 'rate', 'kurds', 'failure', 'bristol', 'caucasus', 'armenian', 'ability', 'pot', 'russian', 'arfa', 'top', 'irregular', 'prevention', 'ambassador', 'power', 'safest', 'one', 'london', 'disaster', 'source', 'eastern', 'races']\n",
      "Soc.Religion.Christian: \n",
      "['ottawa', 'would', 'surprise', 'smileys', 'absolute', 'psuvm', 'hirsch', 'shades', 'rangers', 'team', 'cares', 'american', 'go', 'could', 'one', 'edu', 'space', 'patrick', 'nothing', 'taste', 'eh', 'finnish', 'think', 'lindros', 'hartford']\n",
      "Rec.Sport.Hockey: \n",
      "['people', 'would', 'bible', 'one', 'go', 'think', 'many', 'someone', 'could', 'point', 'religious', 'way', 'christian', 'never', 'look', 'well', 'become', 'said', 'get', 'nothing', 'course', 'jesus', 'reasonable', 'right', 'believe']\n",
      "Comp.Sys.Mac.Hardware: \n",
      "['tony', 'conform', 'pontiac', 'fingers', 'australia', 'warriors', 'would', 'calculations', 'one', 'sentiments', 'european', 'mediocrity', 'aztecs', 'people', 'documentary', 'think', 'conventional', 'nazi', 'ranting', 'get', 'mid', 'billion', 'traded', 'many', 'could']\n",
      "Alt.Atheism: \n",
      "['lindros', 'battalions', 'pinnacle', 'team', 'poorly', 'league', 'surprise', 'rangers', 'corey', 'goalie', 'hartford', 'trade', 'shutout', 'mail', 'maybe', 'quebec', 'earn', 'best', 'keith', 'played', 'season', 'hockey', 'nhl', 'leafs', 'edu']\n"
     ]
    }
   ],
   "source": [
    "for i, true_id in zip(range(len(categories)), res_perm):\n",
    "    print(f'{categories[true_id].title()}: ')\n",
    "    print([id2word[idx[0]] for idx in model.get_topic_terms(topicid=i, topn=25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism',\n",
    " 'comp.sys.mac.hardware']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                      remove=('headers', 'footers', 'quotes'), \n",
    "                                      categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [list(filter(lambda word: not word in ENGLISH_STOP_WORDS, simple_preprocess(piece)))\n",
    "                    for piece in newsgroups_train.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data)\n",
    "texts = data\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "model = LdaModel(corpus, id2word=id2word, \n",
    "                 num_topics=len(categories), iterations=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alt.Atheism: \n",
      "['one', 'god', 'would', 'people', 'know', 'apple', 'think', 'like', 'get', 'anyone', 'also', 'problem', 'new', 'could', 'well', 'good', 'see', 'say', 'mac', 'time', 'jesus', 'even', 'argument', 'true', 'something']\n",
      "Comp.Sys.Mac.Hardware: \n",
      "['one', 'would', 'mac', 'people', 'like', 'system', 'god', 'drive', 'know', 'think', 'use', 'could', 'bit', 'many', 'well', 'mb', 'even', 'time', 'also', 'get', 'way', 'say', 'atheism', 'scsi', 'believe']\n"
     ]
    }
   ],
   "source": [
    "for i, cat in enumerate(categories):\n",
    "    print(f'{cat.title()}: ')\n",
    "    print([id2word[idx[0]] for idx in model.get_topic_terms(topicid=i, topn=25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                     remove=('headers', 'footers', 'quotes'), \n",
    "                                     categories = categories)\n",
    "\n",
    "test_corpus = create_test_corpus(newsgroups_test, ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46579330422125187"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [predict(model, sample)[0] for sample in test_corpus]\n",
    "y_true = newsgroups_test['target'].tolist()\n",
    "\n",
    "d = {0:1, 1:0}\n",
    "new_vec = [d.get(n, n) for n in y_pred]\n",
    "f1_score(y_true, new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5717916137229987"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vladiknaska/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism',\n",
    " 'comp.sys.mac.hardware']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                      remove=('headers', 'footers', 'quotes'), \n",
    "                                      categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [preprocess(text) for text in newsgroups_train.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data)\n",
    "texts = data\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "model = LdaModel(corpus, id2word=id2word, \n",
    "                 num_topics=len(categories), iterations=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                     remove=('headers', 'footers', 'quotes'), \n",
    "                                     categories = categories)\n",
    "\n",
    "test_corpus = create_test_corpus(newsgroups_test, ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [preprocess(text) for text in newsgroups_test.data]\n",
    "id2word = corpora.Dictionary(data)\n",
    "corpus = [id2word.doc2bow(text) for text in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5881006864988557"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [predict(model, sample)[0] for sample in corpus]\n",
    "y_true = newsgroups_test['target'].tolist()\n",
    "\n",
    "d = {0:1, 1:0}\n",
    "new_vec = [d.get(n, n) for n in y_pred]\n",
    "f1_score(y_true, new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
